{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "From HuggingFace: \n",
    "The most basic object in the ðŸ¤— Transformers library is the pipeline() \n",
    "function. It connects a model with its necessary preprocessing and postprocessing \n",
    "steps, allowing us to directly input any text and get an intelligible answer\n",
    "\n",
    "In a nutshell, a pipeline encapsulates a program that executes all the neccesary \n",
    "instructions to perform a task over a given input,in this case: \n",
    "an NLP task.\n",
    "\n",
    "Some available pipelines are:\n",
    "\n",
    "-feature-extraction (get the vector representation of a text)\n",
    "\n",
    "-fill-mask\n",
    "\n",
    "-ner (named entity recognition)\n",
    "\n",
    "-question-answering\n",
    "\n",
    "-sentiment-analysis\n",
    "\n",
    "-summarization\n",
    "\n",
    "-text-generation\n",
    "\n",
    "-translation\n",
    "-zero-shot-classification\n",
    "\n",
    "\n",
    "\n",
    "Resources:\n",
    "\n",
    "\n",
    "[1]https://huggingface.co/learn/nlp-course/chapter1/3?fw=pt\n",
    "\n",
    "[2]https://huggingface.co/docs/transformers/en/tasks/sequence_classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__builtins__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__type_params__']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "[ i for i in dir( pipeline)  ]\n",
    "#pipeline_tasks = pipeline.task\n",
    "#print(pipeline_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is great, the product is broken and they ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.780246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hate everything that is not amazing</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.999703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summer is hot and winter is cold</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.911674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summer is warm and nice but winter is freaking...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.946158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summer is warm and winter is cold</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.858452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is an amazing day</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tiktok sucks</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tiktok piss amazing</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.997794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tiktok is amazing</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trump almost got killed</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.983468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Spain won the cup, I was aiming for England</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.998791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Spain won the cup, England lost</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.907850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence sentiment     score\n",
       "0   this is great, the product is broken and they ...  NEGATIVE  0.780246\n",
       "1               i hate everything that is not amazing  NEGATIVE  0.999703\n",
       "2                    summer is hot and winter is cold  NEGATIVE  0.911674\n",
       "3   summer is warm and nice but winter is freaking...  NEGATIVE  0.946158\n",
       "4                   summer is warm and winter is cold  POSITIVE  0.858452\n",
       "5                              This is an amazing day  POSITIVE  0.999886\n",
       "6                                        tiktok sucks  NEGATIVE  0.997406\n",
       "7                                 tiktok piss amazing  POSITIVE  0.997794\n",
       "8                                   tiktok is amazing  POSITIVE  0.999872\n",
       "9                             Trump almost got killed  NEGATIVE  0.983468\n",
       "10        Spain won the cup, I was aiming for England  POSITIVE  0.998791\n",
       "11                    Spain won the cup, England lost  NEGATIVE  0.907850"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from pprint import pprint as pprint \n",
    "\n",
    "def show_sentiment( pipeline ):\n",
    "    \n",
    "    df  = pd.DataFrame( {} )\n",
    "\n",
    "    df['sentence']=inputs\n",
    "    results = pipeline(inputs)\n",
    "    sentiment = [r['label'] for r in results ]\n",
    "    score = [r['score'] for r in results ]\n",
    "    \n",
    "    df['sentiment'] = sentiment \n",
    "    df['score'] = score\n",
    "\n",
    "    return df #display(df)\n",
    "    \n",
    "def show_num_parameters(model):\n",
    "    print( sum(p.numel() for p in model.parameters()) // 1000000,'Million')\n",
    "\n",
    "\n",
    "\n",
    "inputs  =  ['this is great, the product is broken and they cant replace it',\n",
    "            'i hate everything that is not amazing',\n",
    "            'summer is hot and winter is cold',\n",
    "            'summer is warm and nice but winter is freaking cold',  \n",
    "            'summer is warm and winter is cold',         \n",
    "            'This is an amazing day',\n",
    "            \n",
    "            'tiktok sucks', \n",
    "            'tiktok piss amazing',\n",
    "            'tiktok is amazing',\n",
    "\n",
    "            'Trump almost got killed',\n",
    "            'Spain won the cup, I was aiming for England',\n",
    "            'Spain won the cup, England lost',\n",
    "            \n",
    "            ]\n",
    "\n",
    "default_classifier = pipeline('sentiment-analysis' )\n",
    "show_sentiment( default_classifier )\n",
    "#show_num_parameters(default_classifier.model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['binary_output',\n",
       " 'call_count',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'feature_extractor',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'function_to_apply',\n",
       " 'get_inference_context',\n",
       " 'get_iterator',\n",
       " 'image_processor',\n",
       " 'iterate',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'postprocess',\n",
       " 'predict',\n",
       " 'preprocess',\n",
       " 'push_to_hub',\n",
       " 'return_all_scores',\n",
       " 'run_multi',\n",
       " 'run_single',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'torch_dtype',\n",
       " 'transform']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(default_classifier) if not i.startswith('_') ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is great, the product is broken and they ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.780246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hate everything that is not amazing</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.999703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summer is hot and winter is cold</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.911674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summer is warm and nice but winter is freaking...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.946158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summer is warm and winter is cold</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.858452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is an amazing day</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tiktok sucks</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tiktok piss amazing</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.997794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tiktok is amazing</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trump almost got killed</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.983468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Spain won the cup, I was aiming for England</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.998791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Spain won the cup, England lost</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.907850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence sentiment     score\n",
       "0   this is great, the product is broken and they ...  NEGATIVE  0.780246\n",
       "1               i hate everything that is not amazing  NEGATIVE  0.999703\n",
       "2                    summer is hot and winter is cold  NEGATIVE  0.911674\n",
       "3   summer is warm and nice but winter is freaking...  NEGATIVE  0.946158\n",
       "4                   summer is warm and winter is cold  POSITIVE  0.858452\n",
       "5                              This is an amazing day  POSITIVE  0.999886\n",
       "6                                        tiktok sucks  NEGATIVE  0.997406\n",
       "7                                 tiktok piss amazing  POSITIVE  0.997794\n",
       "8                                   tiktok is amazing  POSITIVE  0.999872\n",
       "9                             Trump almost got killed  NEGATIVE  0.983468\n",
       "10        Spain won the cup, I was aiming for England  POSITIVE  0.998791\n",
       "11                    Spain won the cup, England lost  NEGATIVE  0.907850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "default_model_name = default_classifier.model.name_or_path\n",
    "print(\"Default model:\", default_model_name)\n",
    "\n",
    "default_classifier=pipeline('text-classification',model='distilbert/distilbert-base-uncased-finetuned-sst-2-english')\n",
    "display( show_sentiment( default_classifier ))\n",
    "\n",
    "print( default_classifier.model.config.id2label ) \n",
    "\n",
    "#show_num_parameters(default_classifier.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is great, the product is broken and they ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.554703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hate everything that is not amazing</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.925731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summer is hot and winter is cold</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.598456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summer is warm and nice but winter is freaking...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.482020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summer is warm and winter is cold</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.646170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is an amazing day</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.980470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tiktok sucks</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.907381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tiktok piss amazing</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.883319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tiktok is amazing</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.978076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trump almost got killed</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.791349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Spain won the cup, I was aiming for England</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.517695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Spain won the cup, England lost</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.640999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence sentiment     score\n",
       "0   this is great, the product is broken and they ...  positive  0.554703\n",
       "1               i hate everything that is not amazing  negative  0.925731\n",
       "2                    summer is hot and winter is cold   neutral  0.598456\n",
       "3   summer is warm and nice but winter is freaking...  negative  0.482020\n",
       "4                   summer is warm and winter is cold   neutral  0.646170\n",
       "5                              This is an amazing day  positive  0.980470\n",
       "6                                        tiktok sucks  negative  0.907381\n",
       "7                                 tiktok piss amazing  positive  0.883319\n",
       "8                                   tiktok is amazing  positive  0.978076\n",
       "9                             Trump almost got killed  negative  0.791349\n",
       "10        Spain won the cup, I was aiming for England  positive  0.517695\n",
       "11                    Spain won the cup, England lost   neutral  0.640999"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint ='cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "pipeline_roberta  =pipeline('text-classification',model=checkpoint)\n",
    "show_sentiment( pipeline_roberta )\n",
    "#show_num_parameters(pipeline_roberta.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'neutral', 2: 'positive'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_roberta.model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "context = '''WebGL (Web Graphics Library) is a JavaScript API for rendering high-performance interactive 3D and 2D graphics \n",
    "within any compatible web browser without the use of plug-ins. WebGL does so by introducing an API that closely conforms to \n",
    "OpenGL ES 2.0 that can be used in HTML <canvas> elements. This conformance makes it possible for the API to take advantage of\n",
    "hardware graphics acceleration provided by the user's device.\n",
    "support for WebGL is present in all modern browsers however, the user's device must \n",
    "also have hardware that supports these features.\n",
    "The WebGL 2 API introduces support for much of the OpenGL ES 3.0 feature set; it's provided through the WebGL2RenderingContext \n",
    "interface. The <canvas> element is also used by the Canvas API to do 2D graphics on web pages.'''\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.1561676412820816, 'start': 7, 'end': 27, 'answer': 'Web Graphics Library'}\n",
      "{'score': 0.16372743248939514, 'start': 486, 'end': 557, 'answer': \"the user's device must \\nalso have hardware that supports these features\"}\n",
      "{'score': 0.31525933742523193, 'start': 486, 'end': 557, 'answer': \"the user's device must \\nalso have hardware that supports these features\"}\n",
      "{'score': 0.6059865355491638, 'start': 63, 'end': 110, 'answer': 'high-performance interactive 3D and 2D graphics'}\n"
     ]
    }
   ],
   "source": [
    "print(question_answerer(    question=\"What is WebGL?\",context=context))\n",
    "\n",
    "print(question_answerer(    question=\"Whats the support for WebGL?\",context=context))\n",
    "\n",
    "print(question_answerer(    question=\"Whats the required hardware ?\",context=context))\n",
    "\n",
    "print(question_answerer(    question=\"What are the main features ?\",context=context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "context  = \"\"\"the function named: \"get_current_weather\" is used to fetch the current weather in a given city;\n",
    "              the function named: \"get_exchange_rate\" is used to retrieve currency exchange rate;\n",
    "              the function named: \"plot_locations\" is used to display well locations and to show well locations and to chart well coordinates;\n",
    "              the function:run_model is used to launch a simulation or model, to simulate a model;\n",
    "              \n",
    "              the function named: talk is used to speak to the user and to produce some text and to print something in the screen and to call your daddy \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5968726277351379, 'start': 21, 'end': 40, 'answer': 'get_current_weather'}\n",
      "{'score': 0.5522174835205078, 'start': 229, 'end': 243, 'answer': 'plot_locations'}\n",
      "{'score': 0.6054238677024841, 'start': 229, 'end': 243, 'answer': 'plot_locations'}\n",
      "{'score': 0.6029361486434937, 'start': 131, 'end': 148, 'answer': 'get_exchange_rate'}\n",
      "{'score': 0.7829315662384033, 'start': 364, 'end': 373, 'answer': 'run_model'}\n",
      "{'score': 0.8793507814407349, 'start': 364, 'end': 373, 'answer': 'run_model'}\n",
      "{'score': 0.8569180369377136, 'start': 485, 'end': 489, 'answer': 'talk'}\n",
      "{'score': 0.7482060194015503, 'start': 485, 'end': 489, 'answer': 'talk'}\n",
      "{'score': 0.14781896770000458, 'start': 485, 'end': 489, 'answer': 'talk'}\n",
      "{'score': 0.3525974750518799, 'start': 485, 'end': 489, 'answer': 'talk'}\n"
     ]
    }
   ],
   "source": [
    "print(question_answerer(question=\"What is the function used to get weather in a city  ?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What is the function used to produce a chart of well locations ?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What is the function used to display well locations?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What is the function used to get the currency exchange rate?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What is the function used to launch a simulation?\",context=context))\n",
    "print(question_answerer(question=\"What function should I call to run a simulation?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What is the name of the function used to print text?\",context=context))\n",
    "print(question_answerer(question=\"What is the name of the function used to call daddy?\",context=context))\n",
    "\n",
    "#print(question_answerer(question=\"What is the name of the function used to speak?\",context=context))\n",
    "#print(question_answerer(question=\"What is the function used to show well locations?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What function should I call to print something in the computer screen?\",context=context))\n",
    "\n",
    "print(question_answerer(question=\"What is the function to print something in the computer screen?\",context=context))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind the pipeline \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'default_chat_template',\n",
       " 'deprecation_warnings',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "[x for x in dir( tokenizer ) if not x.startswith('_') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "Help on method add_tokens in module transformers.tokenization_utils_base:\n",
      "\n",
      "add_tokens(new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int method of transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast instance\n",
      "    Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      "    it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n",
      "    algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
      "    not treated in the same way.\n",
      "    \n",
      "    Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
      "    of the model so that its embedding matrix matches the tokenizer.\n",
      "    \n",
      "    In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      "    \n",
      "    Args:\n",
      "        new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
      "            Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
      "            token to let you personalize its behavior: whether this token should only match against a single word,\n",
      "            whether this token should strip all potential whitespaces on the left side, whether this token should\n",
      "            strip all potential whitespaces on the right side, etc.\n",
      "        special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      "            (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      "    \n",
      "            See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      "    \n",
      "    Returns:\n",
      "        `int`: Number of tokens added to the vocabulary.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      "    model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      "    \n",
      "    num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
      "    print(\"We have added\", num_added_toks, \"tokens\")\n",
      "    # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      "    model.resize_token_embeddings(len(tokenizer))\n",
      "    ```\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( tokenizer.vocab_size )\n",
    "\n",
    "print( tokenizer.all_special_tokens )\n",
    "\n",
    "help( tokenizer.add_tokens)\n",
    "len( tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]]),\n",
      " 'input_ids': tensor([[  101,  2188,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "         12172,  2607,  2026,  2878,  2166,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2188,  1045,  5223,  3403,  2061,  2172,   999,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3602,  1010,  2043,  5815,  2047, 19204,  2015,  2000,  1996,\n",
      "         16188,  1010,  2017,  2323,  2191,  2469,  2000,  2036, 24501,  4697,\n",
      "          1996, 19204,  7861,  8270,  4667,  8185,   102]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"home   I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"home   I hate waiting so much!\",\n",
    "    \"Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\"\n",
    "]\n",
    "model_inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "pprint(model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 27, 768])\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**model_inputs)\n",
    "print(outputs.last_hidden_state.shape)  #2 sentences. 17 words each and 768 features per word (embbedings)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "Returns only the model without its head.\n",
    "\n",
    "This architecture contains only the base Transformer module: given some inputs, it outputs what weâ€™ll call hidden states, also known as features. For each model input, weâ€™ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "\n",
    "While these hidden states can be useful on their own, theyâ€™re usually inputs to another part of the model, known as the head. In Chapter 1, the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': ' quel Ã¢ge Ãªtes-vous?'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '- Kakava?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['binary_output',\n",
       " 'call_count',\n",
       " 'check_inputs',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'feature_extractor',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'get_inference_context',\n",
       " 'get_iterator',\n",
       " 'image_processor',\n",
       " 'iterate',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'postprocess',\n",
       " 'predict',\n",
       " 'preprocess',\n",
       " 'push_to_hub',\n",
       " 'return_name',\n",
       " 'run_multi',\n",
       " 'run_single',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'torch_dtype',\n",
       " 'transform']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(translator) if not i.startswith(\"_\") ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TranslationPipeline in module transformers.pipelines.text2text_generation object:\n",
      "\n",
      "class TranslationPipeline(Text2TextGenerationPipeline)\n",
      " |  TranslationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Translates from one language to another.\n",
      " |  \n",
      " |  This translation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"translation_xx_to_yy\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a translation task. See the\n",
      " |  up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=translation).\n",
      " |  For a list of available parameters, see the [following\n",
      " |  documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)\n",
      " |  \n",
      " |  Usage:\n",
      " |  \n",
      " |  ```python\n",
      " |  en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
      " |  en_fr_translator(\"How old are you?\")\n",
      " |  ```\n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TranslationPipeline\n",
      " |      Text2TextGenerationPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Translate the text(s) given as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (`str` or `List[str]`):\n",
      " |              Texts to be translated.\n",
      " |          return_tensors (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to include the tensors of predictions (as token indices) in the outputs.\n",
      " |          return_text (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to include the decoded texts in the outputs.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to clean up the potential extra spaces in the text output.\n",
      " |          src_lang (`str`, *optional*):\n",
      " |              The language of the input. Might be required for multilingual models. Will not have any effect for\n",
      " |              single pair translation models\n",
      " |          tgt_lang (`str`, *optional*):\n",
      " |              The language of the desired output. Might be required for multilingual models. Will not have any effect\n",
      " |              for single pair translation models\n",
      " |          generate_kwargs:\n",
      " |              Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
      " |              corresponding to your framework [here](./model#generative-models)).\n",
      " |      \n",
      " |      Return:\n",
      " |          A list or a list of list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |      \n",
      " |          - **translation_text** (`str`, present when `return_text=True`) -- The translation.\n",
      " |          - **translation_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The\n",
      " |            token ids of the translation.\n",
      " |  \n",
      " |  check_inputs(self, input_length: int, min_length: int, max_length: int)\n",
      " |      Checks whether there might be something wrong with given input with regard to the model.\n",
      " |  \n",
      " |  preprocess(self, *args, truncation=<TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, src_lang=None, tgt_lang=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  return_name = 'translation'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Text2TextGenerationPipeline:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  postprocess(self, model_outputs, return_type=<ReturnType.TEXT: 1>, clean_up_tokenization_spaces=False)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str\n",
      " |      Upload the pipeline file to the ðŸ¤— Model Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`List[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |      \n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |      \n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "ckpt = 'Narrativa/mbart-large-50-finetuned-opus-en-pt-translation'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(ckpt)\n",
    "model = MBartForConditionalGeneration.from_pretrained(ckpt)\n",
    "\n",
    "tokenizer.src_lang = 'en_XX'\n",
    "\n",
    "def translate(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['pt_XX'])\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#text = \"How old are you?\"\n",
    "#translation = translate(text)\n",
    "#print(f\"text = {text}\\ntranslation = {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline \n\u001b[0;32m----> 2\u001b[0m translator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNarrativa/mbart-large-50-finetuned-opus-en-pt-translation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation_en_to_pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#translator(\"How old are you?\", src_lang=\"en\", tgt_lang=\"pt\")\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/modeling_utils.py:3380\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3378\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 3380\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[1;32m   3382\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   3384\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   3385\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   3386\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3387\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   3388\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   3389\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "translator = pipeline(\n",
    "    model=\"Narrativa/mbart-large-50-finetuned-opus-en-pt-translation\", \n",
    "    task=\"translation_en_to_pt\" )\n",
    "\n",
    "\n",
    "#translator(\"How old are you?\", src_lang=\"en\", tgt_lang=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "ckpt = 'Narrativa/mbart-large-50-finetuned-opus-en-pt-translation'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(ckpt)\n",
    "model = MBartForConditionalGeneration.from_pretrained(ckpt)\n",
    "\n",
    "#tokenizer.src_lang = 'en_XX'\n",
    "\n",
    "def translate(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['pt_XX'])\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#text = \"How old are you?\"\n",
    "#translation = translate(text)\n",
    "#print(f\"text = {text}\\ntranslation = {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Helsinki-NLP/opus-mt-tc-big-pt-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-pt-en/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1823\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66a7c721-56d73f147ba7e95356a6dc28;02ac5766-c703-48f2-a4cf-4e957602dd78)\n\nRepository Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-pt-en/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-tc-big-pt-en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(pipe(\">>por<< Tom tried to stab me.\"))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# expected output: O Tom tentou esfaquear-me.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/pipelines/__init__.py:779\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/transformers/utils/hub.py:422\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Helsinki-NLP/opus-mt-tc-big-pt-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-pt\")\n",
    "print(pipe(\">>por<< Tom tried to stab me.\"))\n",
    "\n",
    "# expected output: O Tom tentou esfaquear-me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/MLLearningRepo/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "ckpt = 'Narrativa/mbart-large-50-finetuned-opus-pt-en-translation'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(ckpt)\n",
    "#model = MBartForConditionalGeneration.from_pretrained(ckpt)#.to(\"cuda\")\n",
    "\n",
    "tokenizer.src_lang = 'pt_XX'\n",
    "\n",
    "def translate(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids#.to('cuda')\n",
    "    attention_mask = inputs.attention_mask#.to('cuda')\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['en_XX'])\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "#translate('Tom tentou esfaquear-me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(ckpt)#.to(\"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
